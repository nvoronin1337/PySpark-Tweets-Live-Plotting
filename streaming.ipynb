{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cff9fc3-8b84-4787-a2c9-85870563a5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da69abd0-5925-45b9-982c-d38436d0e47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row,SQLContext\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c64e01eb-e589-47fb-b70b-de8f44831bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/nikita/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b3386d5-5775-43d8-8dae-f9a78abd2632",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sql_context_instance(spark_context):\n",
    "    if ('sqlContextSingletonInstance' not in globals()):\n",
    "        globals()['sqlContextSingletonInstance'] = SQLContext(spark_context)\n",
    "    return globals()['sqlContextSingletonInstance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffc22f29-a551-42bc-ab27-d5579225c813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_data_count(new_values, total_sum):\n",
    "    return sum(new_values) + (total_sum or 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29009bcb-477c-4b1a-b568-fa2fdb7525b1",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "#### Polarity Detection\n",
    "Functions below accept a string and return a polarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5048bd4-369b-46d2-b251-c2fea4eb2408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compound_detection(text):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    return sia.polarity_scores(text)[\"compound\"]\n",
    "\n",
    "def pos_detection(text):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    return sia.polarity_scores(text)[\"pos\"]\n",
    "\n",
    "def neu_detection(text):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    return sia.polarity_scores(text)[\"neu\"]\n",
    "\n",
    "def neg_detection(text):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    return sia.polarity_scores(text)[\"neg\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25ca4f3-d1e1-4b52-aaf2-d6411402c23c",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis\n",
    "Function sentiment_analysis accepts a DataFrame as input.  \n",
    "Creates new column in a DataFrame for each polarity score.\n",
    "Returns updated DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71970c38-e918-4771-ad1d-12a393a0fd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(tweets):\n",
    "    compound_detection_udf = F.udf(compound_detection, StringType())\n",
    "    tweets = tweets.withColumn(\"comp\", compound_detection_udf(\"tweet\"))\n",
    "    \n",
    "    positivity_detection_udf = F.udf(pos_detection, StringType())\n",
    "    tweets = tweets.withColumn(\"pos\", positivity_detection_udf(\"tweet\"))\n",
    "    \n",
    "    neu_detection_udf = F.udf(neu_detection, StringType())\n",
    "    tweets = tweets.withColumn(\"neu\", neu_detection_udf(\"tweet\"))\n",
    "    \n",
    "    neg_detection_udf = F.udf(neg_detection, StringType())\n",
    "    tweets = tweets.withColumn(\"neg\", neg_detection_udf(\"tweet\"))\n",
    "    \n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586e9b6a-b199-4268-803b-ab4198adc863",
   "metadata": {},
   "source": [
    "#### Cleaning Data\n",
    "Function clean_tweets accepts a DataFrame as input.  \n",
    "Cleans text in tweet column of all links, hashtags, mentioned users, as well as several punctuation marks.  \n",
    "Returns updated DataFrame.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ab8baa9-8601-4e95-bb18-af2bd5de4ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(tweets):\n",
    "    tweets = tweets.na.replace('', None)\n",
    "    tweets = tweets.na.drop()\n",
    "\n",
    "    tweets = tweets.withColumn('tweet', F.regexp_replace('tweet', r'http\\S+', ''))\n",
    "    tweets = tweets.withColumn('tweet', F.regexp_replace('tweet', r'(#\\w+)', ''))\n",
    "    tweets = tweets.withColumn('tweet', F.regexp_replace('tweet', r'(@\\w+)', ''))\n",
    "    tweets = tweets.withColumn('tweet', F.regexp_replace('tweet', ':', ''))\n",
    "    tweets = tweets.withColumn('tweet', F.regexp_replace('tweet', 'RT', ''))\n",
    "    tweets = tweets.withColumn('tweet', F.regexp_replace('tweet', r'[^a-zA-Z0-9 -]', ''))\n",
    "    tweets = tweets.withColumn('tweet', F.trim(tweets.tweet))\n",
    "    \n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca19ddc-c2b9-44c0-83b5-b2fce7409a80",
   "metadata": {},
   "source": [
    "#### Send analysis to Flask Application\n",
    "Function send_sentiment_analysis_to_dashboard accepts a DataFrame as input.  \n",
    "Extracts sentiment analysis results.  \n",
    "Computes mean for all polarity values.  \n",
    "Sends values to Flask application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c33d287-1abd-4533-bff6-047ee1d0ea04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_sentiment_analysis_to_dashboard(df):\n",
    "    # \n",
    "    comp_lst = [float(t.comp) for t in df.select(\"comp\").collect()]\n",
    "    comp_mean = sum(comp_lst)/len(comp_lst)\n",
    "    pos_lst = [float(t.pos) for t in df.select(\"pos\").collect()]\n",
    "    pos_mean = sum(pos_lst)/len(pos_lst)\n",
    "    neu_lst = [float(t.neu) for t in df.select(\"neu\").collect()]\n",
    "    neu_mean = sum(neu_lst)/len(neu_lst)\n",
    "    neg_lst = [float(t.neg) for t in df.select(\"neg\").collect()]\n",
    "    neg_mean = sum(neg_lst)/len(neg_lst)\n",
    "    url = 'http://localhost:5001/sentiment/updateData'\n",
    "    request_data = {'comp': str(comp_mean), 'pos': str(pos_mean), 'neu': str(neu_mean), 'neg': str(neg_mean)}\n",
    "    response = requests.post(url, data=request_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc3984b-a8c3-4e12-8042-b9e12a5d4d17",
   "metadata": {},
   "source": [
    "#### Process RDDs for Sentiment Analysis\n",
    "Get spark sql singleton context from the current context.  \n",
    "Convert the RDD to Row RDD.  \n",
    "Create a DF from the Row RDD.  \n",
    "Register the dataframe as table.  \n",
    "Get the top 10 hashtags from the table using SQL.\n",
    "Call this method to prepare comp mean DF and send it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90e830cc-f1f3-444e-9bf8-16d392b1e007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_rdd_sentiment(time, rdd):\n",
    "    print(\"SENTIMENT----------- %s -----------\" % str(time))\n",
    "    try:\n",
    "        sql_context = get_sql_context_instance(rdd.context)\n",
    "        row_rdd = rdd.map(lambda w: Row(tweet=w[0], comp=w[1]))\n",
    "        sentiment_df = sql_context.createDataFrame(row_rdd)\n",
    "        sentiment_df.registerTempTable(\"sentiment\")\n",
    "        sentiment_raw_df = sql_context.sql(\"select tweet, comp from sentiment order by comp desc limit 50\")\n",
    "        sentiment_clean_df = clean_tweets(sentiment_raw_df)\n",
    "        sentiment_analyzed_df = sentiment_analysis(sentiment_clean_df)\n",
    "        send_sentiment_analysis_to_dashboard(sentiment_analyzed_df)\n",
    "    except:\n",
    "        e = sys.exc_info()[0]\n",
    "        print(\"Error: %s\" % e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a539254c-c9ce-4722-b323-fe894fd63167",
   "metadata": {},
   "source": [
    "## Extracting Top Hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335774f8-8b40-4069-9911-0fe7a7d0fa93",
   "metadata": {},
   "source": [
    "#### Send hashtags to Flask Application\n",
    "Extract the hashtags from dataframe and convert them into arraycompound.  \n",
    "Extract the counts from dataframe and convert them into array.  \n",
    "Initialize and send the data through REST API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0122fd13-3727-4ea7-8c14-be3276878d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_hashtag_df_to_dashboard(df):\n",
    "    top_tags = [str(t.hashtag) for t in df.select(\"hashtag\").collect()]\n",
    "    tags_count = [p.hashtag_count for p in df.select(\"hashtag_count\").collect()]\n",
    "    url = 'http://localhost:5001/hashtags/updateData'\n",
    "    request_data = {'label': str(top_tags), 'data': str(tags_count)}\n",
    "    response = requests.post(url, data=request_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcc85ae-20e3-432e-8c83-2334ea83789c",
   "metadata": {},
   "source": [
    "#### Process RDDs for Hashtags\n",
    "Get spark sql singleton context from the current context.  \n",
    "Convert the RDD to Row RDD.  \n",
    "Create a DF from the Row RDD.  \n",
    "Register the dataframe as table.  \n",
    "Get the top 10 hashtags from the table using SQL.\n",
    "Call this method to prepare comp mean DF and send it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e97b689-e585-4f36-a6a4-0581c9f07cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_rdd_hashtags(time, rdd):\n",
    "    print(\"HASHTAG----------- %s -----------\" % str(time))\n",
    "    try:\n",
    "        sql_context = get_sql_context_instance(rdd.context)\n",
    "        row_rdd = rdd.map(lambda w: Row(hashtag=w[0], hashtag_count=w[1]))\n",
    "        hashtags_df = sql_context.createDataFrame(row_rdd)\n",
    "        hashtags_df.registerTempTable(\"hashtags\")\n",
    "        hashtag_counts_df = sql_context.sql(\"select hashtag, hashtag_count from hashtags order by hashtag_count desc limit 20\")\n",
    "        send_hashtag_df_to_dashboard(hashtag_counts_df)\n",
    "    except:\n",
    "        e = sys.exc_info()[0]\n",
    "        print(\"Error: %s\" % e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cc18ef-e625-4e3e-aaea-dbb674472b90",
   "metadata": {},
   "source": [
    "## Extracting Top Mentioned Users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c8ae27-57c8-4775-b7a6-10e0e33aa880",
   "metadata": {},
   "source": [
    "#### Send mentioned users to Flask Application\n",
    "Extract the hashtags from dataframe and convert them into arraycompound.  \n",
    "Extract the counts from dataframe and convert them into array.  \n",
    "Initialize and send the data through REST API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f976a30-edf8-49e7-8196-088108068184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_user_mentions_df_to_dashboard(df):\n",
    "    top_users = [str(t.mention) for t in df.select(\"mention\").collect()]\n",
    "    users_count = [p.mention_count for p in df.select(\"mention_count\").collect()]\n",
    "    url = 'http://localhost:5001/mentions/updateData'\n",
    "    request_data = {'label': str(top_users), 'data': str(users_count)}\n",
    "    response = requests.post(url, data=request_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6d815c-c58b-4720-9ebf-de9ade875d27",
   "metadata": {},
   "source": [
    "#### Process RDDs for Mentioned Users\n",
    "Get spark sql singleton context from the current context.  \n",
    "Convert the RDD to Row RDD.  \n",
    "Create a DF from the Row RDD.  \n",
    "Register the dataframe as table.  \n",
    "Get the top 10 hashtags from the table using SQL.\n",
    "Call this method to prepare comp mean DF and send it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6903c87f-2a27-4748-ae6a-e69bc83268a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_rdd_mentions(time, rdd):\n",
    "    print(\"MENTION----------- %s -----------\" % str(time))\n",
    "    try:\n",
    "        sql_context = get_sql_context_instance(rdd.context)\n",
    "        row_rdd = rdd.map(lambda w: Row(mention=w[0], mention_count=w[1]))\n",
    "        mentions_df = sql_context.createDataFrame(row_rdd)\n",
    "        mentions_df.registerTempTable(\"mentions\")\n",
    "        mentions_count_df = sql_context.sql(\"select mention, mention_count from mentions order by mention_count desc limit 20\")\n",
    "        send_user_mentions_df_to_dashboard(mentions_count_df)\n",
    "    except:\n",
    "        e = sys.exc_info()[0]\n",
    "        print(\"Error: %s\" % e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e3c457-d8e6-4565-911e-a7735732f530",
   "metadata": {},
   "source": [
    "## Spark Structured Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb692b7-9fe6-4b15-a655-08c988418181",
   "metadata": {},
   "source": [
    "#### Setting Up Spark\n",
    "Create spark configurations.  \n",
    "Create spark context with the above configuration.  \n",
    "Create the Streaming Context from the above spark context with interval size 2 seconds.  \n",
    "Setting a checkpoint to allow RDD recovery.  \n",
    "Read data from port 5555."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09e74fb0-99be-437c-8aa1-67f21bfbf534",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/07/30 23:19:37 WARN Utils: Your hostname, nikita-Blade resolves to a loopback address: 127.0.1.1; using 192.168.99.126 instead (on interface wlp2s0)\n",
      "21/07/30 23:19:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "21/07/30 23:19:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf()\n",
    "conf.setAppName(\"TwitterStreamApp\")    # \n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "ssc = StreamingContext(sc, 2)\n",
    "ssc.checkpoint(\"checkpoint_TwitterApp\")\n",
    "dataStream = ssc.socketTextStream(\"0.0.0.0\",5555)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad411f62-a985-443f-bbad-1d6cce854100",
   "metadata": {},
   "source": [
    "#### Initial Parsing of Incoming Data Stream\n",
    "Split each line into tweets.  \n",
    "Split each tweet into words.  \n",
    "Filter the words to get only hashtags/mentions, then map each hashtag/mention to be a pair of (hashtag/mention,1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34f27205-f70f-4c27-a024-cf768e8397f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = dataStream.flatMap(lambda line: line.split(\"t_end\")) \n",
    "words = dataStream.flatMap(lambda text: text.split(\" \"))\n",
    "\n",
    "hashtags = words.filter(lambda w: '#' in w).map(lambda x: (x.upper(), 1))\n",
    "mentions = words.filter(lambda w: '@' in w).map(lambda x: (x.upper(), 1))\n",
    "sentiment = tweets.filter(lambda w: ' ' in w).map(lambda x: (x, 0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6569251b-566c-4541-8c5c-15a02bee34a8",
   "metadata": {},
   "source": [
    "#### Aggregate Date Count\n",
    "Adding the count of each hashtag/mention/sentiment to its previous count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff695c25-00b4-402b-9d6d-bf41b65f1df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_totals = hashtags.updateStateByKey(aggregate_data_count)\n",
    "mentions_totals = mentions.updateStateByKey(aggregate_data_count)\n",
    "sentiment_totals = sentiment.updateStateByKey(aggregate_data_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c13cd3b-ea30-4828-977d-81937b6a310b",
   "metadata": {},
   "source": [
    "#### Process RDDs\n",
    "Do processing for each RDD generated in each interval.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3277c088-2762-4cc0-b95c-a9be3e0ce167",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_totals.foreachRDD(process_rdd_hashtags)\n",
    "mentions_totals.foreachRDD(process_rdd_mentions)\n",
    "sentiment_totals.foreachRDD(process_rdd_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de333c59-d3a7-4a3a-ae47-351779a3e2f1",
   "metadata": {},
   "source": [
    "#### Start Streaming\n",
    "Start the streaming computation.  \n",
    "Wait for the streaming to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e698961-42d8-4b5c-a53a-253e655f87fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HASHTAG----------- 2021-07-30 23:19:40 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: <class 'ValueError'>\n",
      "MENTION----------- 2021-07-30 23:19:40 -----------\n",
      "Error: <class 'ValueError'>\n",
      "SENTIMENT----------- 2021-07-30 23:19:40 -----------\n",
      "Error: <class 'ValueError'>\n",
      "HASHTAG----------- 2021-07-30 23:19:42 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MENTION----------- 2021-07-30 23:19:42 -----------\n",
      "SENTIMENT----------- 2021-07-30 23:19:42 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HASHTAG----------- 2021-07-30 23:19:44 -----------\n",
      "MENTION----------- 2021-07-30 23:19:44 -----------\n",
      "SENTIMENT----------- 2021-07-30 23:19:44 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HASHTAG----------- 2021-07-30 23:19:46 -----------\n",
      "MENTION----------- 2021-07-30 23:19:46 -----------\n",
      "SENTIMENT----------- 2021-07-30 23:19:46 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/07/30 23:19:58 ERROR ReceiverTracker: Deregistered receiver for stream 0: Stopped by driver\n",
      "Exception in thread \"receiver-supervisor-future-0\" java.lang.Error: java.lang.InterruptedException: sleep interrupted\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1155)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.lang.InterruptedException: sleep interrupted\n",
      "\tat java.lang.Thread.sleep(Native Method)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:196)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\t... 2 more\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HASHTAG----------- 2021-07-30 23:19:48 -----------\n",
      "MENTION----------- 2021-07-30 23:19:48 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/07/30 23:20:00 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@6adf8f1e rejected from java.util.concurrent.ThreadPoolExecutor@6b82ff45[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 447]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:769)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:745)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "21/07/30 23:20:00 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@3196c295 rejected from java.util.concurrent.ThreadPoolExecutor@6b82ff45[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 447]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:769)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:745)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "21/07/30 23:20:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:642)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1423)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1350)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1414)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1237)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "21/07/30 23:20:00 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@43a413d9 rejected from java.util.concurrent.ThreadPoolExecutor@6b82ff45[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 447]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:769)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:745)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "21/07/30 23:20:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:550)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:539)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:657)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:621)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:397)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:232)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:642)\n",
      "\t... 15 more\n",
      "21/07/30 23:20:00 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@45ae75aa rejected from java.util.concurrent.ThreadPoolExecutor@6b82ff45[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 447]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:769)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:745)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "21/07/30 23:20:00 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@1f8ea36a rejected from java.util.concurrent.ThreadPoolExecutor@6b82ff45[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 447]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:769)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:745)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "21/07/30 23:20:00 ERROR Executor: Exception in task 5.0 in stage 188.0 (TID 451): Python worker exited unexpectedly (crashed)\n",
      "21/07/30 23:20:00 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@6220800f rejected from java.util.concurrent.ThreadPoolExecutor@6b82ff45[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 447]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:769)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:745)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "21/07/30 23:20:00 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@174370b7 rejected from java.util.concurrent.ThreadPoolExecutor@6b82ff45[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 447]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:769)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:745)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "21/07/30 23:20:00 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@4ce1eceb rejected from java.util.concurrent.ThreadPoolExecutor@6b82ff45[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 447]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:769)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:745)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "21/07/30 23:20:00 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@7718d82f rejected from java.util.concurrent.ThreadPoolExecutor@6b82ff45[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 447]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:769)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:745)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "21/07/30 23:20:00 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@66b4f345 rejected from java.util.concurrent.ThreadPoolExecutor@6b82ff45[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 447]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:769)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:745)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: <class 'py4j.protocol.Py4JJavaError'>\n",
      "Interrupted\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ssc.start()\n",
    "    ssc.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    ssc.stop()\n",
    "    print('Interrupted')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
