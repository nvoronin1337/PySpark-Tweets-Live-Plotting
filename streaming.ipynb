{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import requests\n",
    "import sys"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row,SQLContext\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import functions as F"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_sql_context_instance(spark_context):\n",
    "    if ('sqlContextSingletonInstance' not in globals()):\n",
    "        globals()['sqlContextSingletonInstance'] = SQLContext(spark_context)\n",
    "    return globals()['sqlContextSingletonInstance']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def aggregate_data_count(new_values, total_sum):\n",
    "    return sum(new_values) + (total_sum or 0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "#### Polarity Detection\n",
    "Functions below accept a string and return a polarity score."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def compound_detection(text):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    return sia.polarity_scores(text)[\"compound\"]\n",
    "\n",
    "def pos_detection(text):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    return sia.polarity_scores(text)[\"pos\"]\n",
    "\n",
    "def neu_detection(text):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    return sia.polarity_scores(text)[\"neu\"]\n",
    "\n",
    "def neg_detection(text):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    return sia.polarity_scores(text)[\"neg\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Sentiment Analysis\n",
    "Function sentiment_analysis accepts a DataFrame as input.  \n",
    "Creates new column in a DataFrame for each polarity score.\n",
    "Returns updated DataFrame."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def sentiment_analysis(tweets):\n",
    "    compound_detection_udf = F.udf(compound_detection, StringType())\n",
    "    tweets = tweets.withColumn(\"comp\", compound_detection_udf(\"tweet\"))\n",
    "    \n",
    "    positivity_detection_udf = F.udf(pos_detection, StringType())\n",
    "    tweets = tweets.withColumn(\"pos\", positivity_detection_udf(\"tweet\"))\n",
    "    \n",
    "    neu_detection_udf = F.udf(neu_detection, StringType())\n",
    "    tweets = tweets.withColumn(\"neu\", neu_detection_udf(\"tweet\"))\n",
    "    \n",
    "    neg_detection_udf = F.udf(neg_detection, StringType())\n",
    "    tweets = tweets.withColumn(\"neg\", neg_detection_udf(\"tweet\"))\n",
    "    \n",
    "    return tweets"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Cleaning Data\n",
    "Function clean_tweets accepts a DataFrame as input.  \n",
    "Cleans text in tweet column of all links, hashtags, mentioned users, as well as several punctuation marks.  \n",
    "Returns updated DataFrame.  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def clean_tweets(tweets):\n",
    "    tweets = tweets.na.replace('', None)\n",
    "    tweets = tweets.na.drop()\n",
    "\n",
    "    tweets = tweets.withColumn('tweet', F.regexp_replace('tweet', r'http\\S+', ''))\n",
    "    tweets = tweets.withColumn('tweet', F.regexp_replace('tweet', r'(#\\w+)', ''))\n",
    "    tweets = tweets.withColumn('tweet', F.regexp_replace('tweet', r'(@\\w+)', ''))\n",
    "    tweets = tweets.withColumn('tweet', F.regexp_replace('tweet', ':', ''))\n",
    "    tweets = tweets.withColumn('tweet', F.regexp_replace('tweet', 'RT', ''))\n",
    "    tweets = tweets.withColumn('tweet', F.regexp_replace('tweet', r'[^a-zA-Z0-9 -]', ''))\n",
    "    tweets = tweets.withColumn('tweet', F.trim(tweets.tweet))\n",
    "    \n",
    "    return tweets"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Send analysis to Flask Application\n",
    "Function send_sentiment_analysis_to_dashboard accepts a DataFrame as input.  \n",
    "Extracts sentiment analysis results.  \n",
    "Computes mean for all polarity values.  \n",
    "Sends values to Flask application."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def send_sentiment_analysis_to_dashboard(df):\n",
    "    # \n",
    "    comp_lst = [float(t.comp) for t in df.select(\"comp\").collect()]\n",
    "    comp_mean = sum(comp_lst)/len(comp_lst)\n",
    "    pos_lst = [float(t.pos) for t in df.select(\"pos\").collect()]\n",
    "    pos_mean = sum(pos_lst)/len(pos_lst)\n",
    "    neu_lst = [float(t.neu) for t in df.select(\"neu\").collect()]\n",
    "    neu_mean = sum(neu_lst)/len(neu_lst)\n",
    "    neg_lst = [float(t.neg) for t in df.select(\"neg\").collect()]\n",
    "    neg_mean = sum(neg_lst)/len(neg_lst)\n",
    "    url = 'http://localhost:5001/sentiment/updateData'\n",
    "    request_data = {'comp': str(comp_mean), 'pos': str(pos_mean), 'neu': str(neu_mean), 'neg': str(neg_mean)}\n",
    "    response = requests.post(url, data=request_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Process RDDs for Sentiment Analysis\n",
    "Get spark sql singleton context from the current context.  \n",
    "Convert the RDD to Row RDD.  \n",
    "Create a DF from the Row RDD.  \n",
    "Register the dataframe as table.  \n",
    "Get the top 10 hashtags from the table using SQL.\n",
    "Call this method to prepare comp mean DF and send it."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def process_rdd_sentiment(time, rdd):\n",
    "    print(\"SENTIMENT----------- %s -----------\" % str(time))\n",
    "    try:\n",
    "        sql_context = get_sql_context_instance(rdd.context)\n",
    "        row_rdd = rdd.map(lambda w: Row(tweet=w[0], comp=w[1]))\n",
    "        sentiment_df = sql_context.createDataFrame(row_rdd)\n",
    "        sentiment_df.registerTempTable(\"sentiment\")\n",
    "        sentiment_raw_df = sql_context.sql(\"select tweet, comp from sentiment order by comp desc limit 50\")\n",
    "        sentiment_clean_df = clean_tweets(sentiment_raw_df)\n",
    "        sentiment_analyzed_df = sentiment_analysis(sentiment_clean_df)\n",
    "        send_sentiment_analysis_to_dashboard(sentiment_analyzed_df)\n",
    "    except:\n",
    "        e = sys.exc_info()[0]\n",
    "        print(\"Error: %s\" % e)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extracting Top Hashtags"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Send hashtags to Flask Application\n",
    "Extract the hashtags from dataframe and convert them into arraycompound.  \n",
    "Extract the counts from dataframe and convert them into array.  \n",
    "Initialize and send the data through REST API."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def send_hashtag_df_to_dashboard(df):\n",
    "    top_tags = [str(t.hashtag) for t in df.select(\"hashtag\").collect()]\n",
    "    tags_count = [p.hashtag_count for p in df.select(\"hashtag_count\").collect()]\n",
    "    url = 'http://localhost:5001/hashtags/updateData'\n",
    "    request_data = {'label': str(top_tags), 'data': str(tags_count)}\n",
    "    response = requests.post(url, data=request_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Process RDDs for Hashtags\n",
    "Get spark sql singleton context from the current context.  \n",
    "Convert the RDD to Row RDD.  \n",
    "Create a DF from the Row RDD.  \n",
    "Register the dataframe as table.  \n",
    "Get the top 10 hashtags from the table using SQL.\n",
    "Call this method to prepare comp mean DF and send it."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def process_rdd_hashtags(time, rdd):\n",
    "    print(\"HASHTAG----------- %s -----------\" % str(time))\n",
    "    try:\n",
    "        sql_context = get_sql_context_instance(rdd.context)\n",
    "        row_rdd = rdd.map(lambda w: Row(hashtag=w[0], hashtag_count=w[1]))\n",
    "        hashtags_df = sql_context.createDataFrame(row_rdd)\n",
    "        hashtags_df.registerTempTable(\"hashtags\")\n",
    "        hashtag_counts_df = sql_context.sql(\"select hashtag, hashtag_count from hashtags order by hashtag_count desc limit 20\")\n",
    "        send_hashtag_df_to_dashboard(hashtag_counts_df)\n",
    "    except:\n",
    "        e = sys.exc_info()[0]\n",
    "        print(\"Error: %s\" % e)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extracting Top Mentioned Users"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Send mentioned users to Flask Application\n",
    "Extract the hashtags from dataframe and convert them into arraycompound.  \n",
    "Extract the counts from dataframe and convert them into array.  \n",
    "Initialize and send the data through REST API."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def send_user_mentions_df_to_dashboard(df):\n",
    "    top_users = [str(t.mention) for t in df.select(\"mention\").collect()]\n",
    "    users_count = [p.mention_count for p in df.select(\"mention_count\").collect()]\n",
    "    url = 'http://localhost:5001/mentions/updateData'\n",
    "    request_data = {'label': str(top_users), 'data': str(users_count)}\n",
    "    response = requests.post(url, data=request_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Process RDDs for Mentioned Users\n",
    "Get spark sql singleton context from the current context.  \n",
    "Convert the RDD to Row RDD.  \n",
    "Create a DF from the Row RDD.  \n",
    "Register the dataframe as table.  \n",
    "Get the top 10 hashtags from the table using SQL.\n",
    "Call this method to prepare comp mean DF and send it."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def process_rdd_mentions(time, rdd):\n",
    "    print(\"MENTION----------- %s -----------\" % str(time))\n",
    "    try:\n",
    "        sql_context = get_sql_context_instance(rdd.context)\n",
    "        row_rdd = rdd.map(lambda w: Row(mention=w[0], mention_count=w[1]))\n",
    "        mentions_df = sql_context.createDataFrame(row_rdd)\n",
    "        mentions_df.registerTempTable(\"mentions\")\n",
    "        mentions_count_df = sql_context.sql(\"select mention, mention_count from mentions order by mention_count desc limit 20\")\n",
    "        send_user_mentions_df_to_dashboard(mentions_count_df)\n",
    "    except:\n",
    "        e = sys.exc_info()[0]\n",
    "        print(\"Error: %s\" % e)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Spark Structured Streaming"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Setting Up Spark\n",
    "Create spark configurations.  \n",
    "Create spark context with the above configuration.  \n",
    "Create the Streaming Context from the above spark context with interval size 2 seconds.  \n",
    "Setting a checkpoint to allow RDD recovery.  \n",
    "Read data from port 5555."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "conf = SparkConf()\n",
    "conf.setAppName(\"TwitterStreamApp\")    # \n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "ssc = StreamingContext(sc, 2)\n",
    "ssc.checkpoint(\"checkpoint_TwitterApp\")\n",
    "dataStream = ssc.socketTextStream(\"0.0.0.0\",5555)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Initial Parsing of Incoming Data Stream\n",
    "Split each line into tweets.  \n",
    "Split each tweet into words.  \n",
    "Filter the words to get only hashtags/mentions, then map each hashtag/mention to be a pair of (hashtag/mention,1)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tweets = dataStream.flatMap(lambda line: line.split(\"t_end\")) \n",
    "words = dataStream.flatMap(lambda text: text.split(\" \"))\n",
    "\n",
    "hashtags = words.filter(lambda w: '#' in w).map(lambda x: (x.upper(), 1))\n",
    "mentions = words.filter(lambda w: '@' in w).map(lambda x: (x.upper(), 1))\n",
    "sentiment = tweets.filter(lambda w: ' ' in w).map(lambda x: (x, 0.0))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Aggregate Date Count\n",
    "Adding the count of each hashtag/mention/sentiment to its previous count"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "hashtags_totals = hashtags.updateStateByKey(aggregate_data_count)\n",
    "mentions_totals = mentions.updateStateByKey(aggregate_data_count)\n",
    "sentiment_totals = sentiment.updateStateByKey(aggregate_data_count)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Process RDDs\n",
    "Do processing for each RDD generated in each interval.  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "hashtags_totals.foreachRDD(process_rdd_hashtags)\n",
    "mentions_totals.foreachRDD(process_rdd_mentions)\n",
    "sentiment_totals.foreachRDD(process_rdd_sentiment)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Start Streaming\n",
    "Start the streaming computation.  \n",
    "Wait for the streaming to finish."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "try:\n",
    "    ssc.start()\n",
    "    ssc.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    ssc.stop()\n",
    "    print('Interrupted')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}